import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
import os

class MasothueCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.results = []
    
    def extract_company_info(self, url):
        """Tr√≠ch xu·∫•t th√¥ng tin c√¥ng ty t·ª´ m·ªôt URL masothue.com"""
        try:
            print(f"ƒêang crawl: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # T√¨m b·∫£ng ch·ª©a th√¥ng tin c√¥ng ty
            tbody = soup.find('tbody')
            if not tbody:
                print(f"Kh√¥ng t√¨m th·∫•y b·∫£ng th√¥ng tin cho URL: {url}")
                return None
            
            # Dictionary ƒë·ªÉ l∆∞u th√¥ng tin
            company_info = {
                'URL': url,
                'M√£ s·ªë thu·∫ø': '',
                'ƒê·ªãa ch·ªâ': '',
                'Ng∆∞·ªùi ƒë·∫°i di·ªán': '',
                'ƒêi·ªán tho·∫°i': '',
                'Ng√†y ho·∫°t ƒë·ªông': '',
                'Qu·∫£n l√Ω b·ªüi': '',
                'Lo·∫°i h√¨nh DN': '',
                'T√¨nh tr·∫°ng': '',
                'Ng√†nh ngh·ªÅ ch√≠nh': ''
            }
            
            # L·∫•y th√¥ng tin t·ª´ c√°c h√†ng trong b·∫£ng
            rows = tbody.find_all('tr')
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    # L·∫•y text c·ªßa c·ªôt ƒë·∫ßu ti√™n (label)
                    label_cell = cells[0]
                    value_cell = cells[1]
                    
                    # Lo·∫°i b·ªè icon v√† l·∫•y text
                    label_text = label_cell.get_text(strip=True)
                    
                    # X·ª≠ l√Ω t·ª´ng lo·∫°i th√¥ng tin
                    if 'M√£ s·ªë thu·∫ø' in label_text:
                        span = value_cell.find('span', class_='copy')
                        if span:
                            company_info['M√£ s·ªë thu·∫ø'] = span.get_text(strip=True)
                    
                    elif 'ƒê·ªãa ch·ªâ' in label_text:
                        span = value_cell.find('span', class_='copy')
                        if span:
                            company_info['ƒê·ªãa ch·ªâ'] = span.get_text(strip=True)
                    
                    elif 'Ng∆∞·ªùi ƒë·∫°i di·ªán' in label_text:
                        # T√¨m t√™n trong th·∫ª span ho·∫∑c a
                        name_element = value_cell.find('span', itemprop='name')
                        if name_element:
                            a_tag = name_element.find('a')
                            if a_tag:
                                company_info['Ng∆∞·ªùi ƒë·∫°i di·ªán'] = a_tag.get_text(strip=True)
                            else:
                                company_info['Ng∆∞·ªùi ƒë·∫°i di·ªán'] = name_element.get_text(strip=True)
                    
                    elif 'ƒêi·ªán tho·∫°i' in label_text:
                        span = value_cell.find('span', class_='copy')
                        if span:
                            company_info['ƒêi·ªán tho·∫°i'] = span.get_text(strip=True)
                    
                    elif 'Ng√†y ho·∫°t ƒë·ªông' in label_text:
                        span = value_cell.find('span', class_='copy')
                        if span:
                            company_info['Ng√†y ho·∫°t ƒë·ªông'] = span.get_text(strip=True)
                    
                    elif 'Qu·∫£n l√Ω b·ªüi' in label_text:
                        span = value_cell.find('span', class_='copy')
                        if span:
                            company_info['Qu·∫£n l√Ω b·ªüi'] = span.get_text(strip=True)
                    
                    elif 'Lo·∫°i h√¨nh DN' in label_text:
                        a_tag = value_cell.find('a')
                        if a_tag:
                            company_info['Lo·∫°i h√¨nh DN'] = a_tag.get_text(strip=True)
                    
                    elif 'T√¨nh tr·∫°ng' in label_text:
                        a_tag = value_cell.find('a')
                        if a_tag:
                            company_info['T√¨nh tr·∫°ng'] = a_tag.get_text(strip=True)
                    
                    elif 'Ng√†nh ngh·ªÅ ch√≠nh' in label_text:
                        # L·∫•y to√†n b·ªô text, bao g·ªìm c·∫£ m√¥ t·∫£ trong ngo·∫∑c
                        text_content = value_cell.get_text(separator=' ', strip=True)
                        # Lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng th·ª´a
                        company_info['Ng√†nh ngh·ªÅ ch√≠nh'] = re.sub(r'\s+', ' ', text_content)
            
            print(f"‚úì Crawl th√†nh c√¥ng: {company_info['M√£ s·ªë thu·∫ø']}")
            return company_info
            
        except Exception as e:
            print(f"‚úó L·ªói khi crawl {url}: {str(e)}")
            return None
    
    def load_urls_from_file(self, filename='urls.txt'):
        """ƒê·ªçc danh s√°ch URL t·ª´ file v√† lo·∫°i b·ªè URL tr√πng l·∫∑p"""
        urls = []
        seen_urls = set()
        duplicate_count = 0
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    # B·ªè qua d√≤ng tr·ªëng v√† d√≤ng comment
                    if line and not line.startswith('#'):
                        if line.startswith('https://masothue.com'):
                            if line in seen_urls:
                                duplicate_count += 1
                                print(f"‚ö† URL tr√πng l·∫∑p (d√≤ng {line_num}): {line}")
                            else:
                                seen_urls.add(line)
                                urls.append(line)
                        else:
                            print(f"‚ö† B·ªè qua URL kh√¥ng h·ª£p l·ªá (d√≤ng {line_num}): {line}")
        except FileNotFoundError:
            print(f"‚úó Kh√¥ng t√¨m th·∫•y file {filename}")
            return []
        except Exception as e:
            print(f"‚úó L·ªói khi ƒë·ªçc file {filename}: {str(e)}")
            return []
        
        if duplicate_count > 0:
            print(f"üìä ƒê√£ lo·∫°i b·ªè {duplicate_count} URL tr√πng l·∫∑p")
        
        return urls
    
    def crawl_multiple_urls(self, urls):
        """Crawl nhi·ªÅu URL v√† l∆∞u k·∫øt qu·∫£"""
        # Ki·ªÉm tra v√† th√¥ng b√°o v·ªÅ URL tr√πng l·∫∑p tr∆∞·ªõc khi crawl
        original_count = len(urls)
        unique_urls = list(dict.fromkeys(urls))  # Gi·ªØ th·ª© t·ª± v√† lo·∫°i b·ªè tr√πng l·∫∑p
        
        if len(unique_urls) < original_count:
            duplicate_count = original_count - len(unique_urls)
            print(f"üîç Ph√°t hi·ªán {duplicate_count} URL tr√πng l·∫∑p, ch·ªâ crawl {len(unique_urls)} URL duy nh·∫•t")
            urls = unique_urls
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] ƒêang x·ª≠ l√Ω...")
            
            info = self.extract_company_info(url)
            if info:
                self.results.append(info)
            
            # Th√™m delay ƒë·ªÉ tr√°nh b·ªã block
            time.sleep(2)
        
        print(f"\n‚úì Ho√†n th√†nh! ƒê√£ crawl th√†nh c√¥ng {len(self.results)}/{len(urls)} c√¥ng ty.")
    
    def save_to_excel(self, filename='masothue_data.xlsx'):
        """L∆∞u k·∫øt qu·∫£ ra file Excel"""
        if not self.results:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u!")
            return
        
        df = pd.DataFrame(self.results)
        
        # S·∫Øp x·∫øp c√°c c·ªôt theo th·ª© t·ª± mong mu·ªën
        column_order = [
            'M√£ s·ªë thu·∫ø', 'ƒê·ªãa ch·ªâ', 'Ng∆∞·ªùi ƒë·∫°i di·ªán', 'ƒêi·ªán tho·∫°i', 
            'Ng√†y ho·∫°t ƒë·ªông', 'Qu·∫£n l√Ω b·ªüi', 'Lo·∫°i h√¨nh DN', 
            'T√¨nh tr·∫°ng', 'Ng√†nh ngh·ªÅ ch√≠nh', 'URL'
        ]
        
        df = df[column_order]
        
        # L∆∞u ra file Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"‚úì ƒê√£ l∆∞u d·ªØ li·ªáu ra file: {filename}")
        print(f"T·ªïng s·ªë c√¥ng ty: {len(self.results)}")
    
    def check_duplicate_urls(self, filename='urls.txt'):
        """Ki·ªÉm tra v√† b√°o c√°o URL tr√πng l·∫∑p trong file"""
        urls = []
        url_lines = {}  # Dictionary ƒë·ªÉ l∆∞u URL v√† s·ªë d√≤ng
        duplicates = []
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if line.startswith('https://masothue.com'):
                            if line in url_lines:
                                duplicates.append({
                                    'url': line,
                                    'first_line': url_lines[line],
                                    'duplicate_line': line_num
                                })
                            else:
                                url_lines[line] = line_num
                            urls.append(line)
            
            print(f"üìä K·∫øt qu·∫£ ki·ªÉm tra file {filename}:")
            print(f"   - T·ªïng s·ªë URL: {len(urls)}")
            print(f"   - URL duy nh·∫•t: {len(url_lines)}")
            print(f"   - URL tr√πng l·∫∑p: {len(duplicates)}")
            
            if duplicates:
                print("\nüîç Chi ti·∫øt URL tr√πng l·∫∑p:")
                for dup in duplicates:
                    print(f"   - D√≤ng {dup['duplicate_line']}: tr√πng v·ªõi d√≤ng {dup['first_line']}")
                    print(f"     URL: {dup['url']}")
            else:
                print("‚úì Kh√¥ng c√≥ URL tr√πng l·∫∑p!")
                
            return duplicates
            
        except FileNotFoundError:
            print(f"‚úó Kh√¥ng t√¨m th·∫•y file {filename}")
            return []
        except Exception as e:
            print(f"‚úó L·ªói khi ki·ªÉm tra file {filename}: {str(e)}")
            return []

def main():
    print("=== MASOTHUE.COM CRAWLER ===")
    
    # T·∫°o crawler
    crawler = MasothueCrawler()
    
    # Ki·ªÉm tra URL tr√πng l·∫∑p tr∆∞·ªõc
    print("\nüîç Ki·ªÉm tra URL tr√πng l·∫∑p...")
    crawler.check_duplicate_urls('urls.txt')
    
    # ƒê·ªçc URL t·ª´ file
    urls = crawler.load_urls_from_file('urls.txt')
    
    if not urls:
        print("Kh√¥ng c√≥ URL n√†o ƒë·ªÉ crawl!")
        print("Vui l√≤ng th√™m URL v√†o file 'urls.txt'")
        return
    
    print(f"\nS·ªë l∆∞·ª£ng URL duy nh·∫•t c·∫ßn crawl: {len(urls)}")
    
    # Hi·ªÉn th·ªã danh s√°ch URL
    print("\nDanh s√°ch URL s·∫Ω ƒë∆∞·ª£c crawl:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    
    # X√°c nh·∫≠n tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    response = input(f"\nB·∫°n c√≥ mu·ªën ti·∫øp t·ª•c crawl {len(urls)} URL? (y/n): ").strip().lower()
    if response not in ['y', 'yes', 'c√≥']:
        print("ƒê√£ h·ªßy!")
        return
    
    # B·∫Øt ƒë·∫ßu crawl
    crawler.crawl_multiple_urls(urls)
    
    # L∆∞u k·∫øt qu·∫£ ra Excel
    if crawler.results:
        # T·∫°o t√™n file v·ªõi timestamp
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"masothue_companies_{timestamp}.xlsx"
        crawler.save_to_excel(filename)
    
    print("\n=== HO√ÄN TH√ÄNH ===")


if __name__ == "__main__":
    main()
